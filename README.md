# 红包系统设计文档
![架构图](/imgs/1.png)

### 业务逻辑层
```
    业务逻辑层设计成无状态服务，数据的一致性，全部依赖数据库的事务操作，也就是说数据库的性能决定了整个系统的性能。
    好消息是，我们拥有一套支持水平扩展的扩容方案（后面再详细介绍），单机数据库的不足将由多台服务器所弥补。
```

### 异步设计
```
    运用了kafka队列，对一些无关的业务，尽可能异步去处理，比如抢到红包的到账业务，用户一般不会马上去钱包查看明细。
    Kafka采用集群搭建，备份因子>=2，一条消息，同时写入两台kafka才算成功，保证消息不丢失。
```

### 冷热数据迁移
``` 
    红包数据的访问热度，随着时间流逝会急剧降低，也就是数据的访问时间段非常集中，一般红包发出三天后，
    99%的用户不会再去点开这个红包了。因此红包系统采取按时间做冷热数据分离，降低数据的存储成本，同时提升了热数据的访问性能。
```

### 数据存储
![数据set架构](/imgs/2.png)
```
    每一个数据set，会分配一个唯一的两位数编号00-99。同时，每一个set拥有编号0-9的物理数据库。
    也就说，最终我们的红包集群最多能有100*10=1000数据库。
    假设一个库处理5W的热数据，就能最多同时处理5千万的红包热数据（肯定够用了......）
```

### 红包id的设计
```
    红包id保证全局唯一，同时，红包id的最后三位数格式为XXY，代表了它具体落地的数据库。
    例如1928741798371917037，表示该红包所在数据set为03，具体数据库编号为7。
    通过这样的设计，给定任何一个红包id，我们都可以定位它所在的物理数据库。
```

### 扩容(缩容)方案
![扩容方案](/imgs/3.png)
```
    在这套设计中，多人争抢同一个红包，数据的一致性严格依赖数据库事务，必然会大量存在MYSQL行锁竞争，当请求量达到一定级别，单台数据库事务必将变得越来越缓慢，拖慢整个系统的效率。
    因此，一个安全平稳的水平拓展方案变得非常重要。
    从整个架构设计图中我们看到引用了zookeeper，它能够提供一种全局配置的管理。业务逻辑层依赖zookeeper来确定红包的路由规则。
    假设当前系统有00-03四个数据set，当我们需要引入编号为04的set时，我们只需要将服务部署好，然后向zookeeper添加一条关于04set的配置信息，上面附带它的rpc地址和端口。
    业务层收到zookeeper的通知后便开始生成尾数为04Y的红包id，水平扩容便完成了，甚至不需要重启原有服务器（真666）。
    缩容的原理和扩容类似，只需要删除指定的set信息配置为不可用，业务层将不再生成对应set的红包，待原有红包自然过期后，即可移除该set。
    随着时间的推进，冷热数据迁移，最终各个set中的数据压力将趋于平衡。
```

### 熔断保护
![熔断保护](/imgs/4.png)
```
    由于业务层无状态，它可能和任何一个数据set交互，而数据set实际上就是操作MYSQL数据库，因此某一个数据set变得缓慢或者不可用，将会影响整个集群。
    解决的办法是，数据set主动监测DB的性能情况，或者是自身的CPU情况，当DB性能严重下降或者CPU占用率飙高，数据set将执行快速拒绝服务，业务层收到拒绝后则尝试访问其他数据set。
    或者当MYSQL在规定时间内，产生一定数量的错误，也拒绝服务一段时间，等待DB自愈。
    熔断期间，该数据set中已经入库的红包将变得不可领取。
```

### 数据库表设计优化
```
    数据表除了水平切分(分库分表)，行内数据也可以按属性进一步分开（垂直拆分）。
    核心表只保留最关键的字段，保证数据文件短小紧凑。
    以红包为例，祝福语和拓展字段这类较长的信息，不属于核心数据，另外还有发送方、接收方、创建时间、过期时间等静态数据，完全可以切分到别的机器或者别的库上，进一步提升核心数据库的容量。
    另外还有一个好处是，短小的行数据更新操作将更加快速，大大增加事务的处理能力。
    不同数据适合的存储类型也不一样，这类重复率高的长字符串更适合NoSQL存储，对存储空间和性能都是节约极大。在冷热数据迁移时，再将各表数据和NoSQL数据整合迁移。
```

### 如何处理range查询
```
    由于数据按set进行落地，而每个set又有10个物理数据库。因此，range筛选将变得非常困难
    比如要筛选某时间A到时间B之间的红包、或者是筛选某个用户发出的所有红包、或者筛选某个群内的红包
    这类筛选势必要遍历所有的set以及所有的物理库甚至每个物理库的逻辑表（代价太大太大了）
```
![一次range查询](/imgs/5.png)
```
    如何解决呢？
    一个折衷的方案是，创建一个“近期红包数据”的数据库，并且采用单库单表设计，
    该库保留近一周或者一月的数据，平时做统计或者后台查看红包列表的时候，读取近期红包数据。
    另外还可以按月或者按年生成报表形式存储数据，以后直接查看报表
```

### 高可用设计
```    
    目前为止，我们实现了数据set化，set化后可以无缝扩容缩容。
    但是当一台数据set宕机后，不可避免的还是造成一批红包无法被访问。
    解决方案是数据set做主备，甚至可以一主多备。
    假设有AB两台数据set，它们同时都维护着编号为00的数据。启动时，AB同时向zookeeper注册一个临时节点，注册成功的即为主，注册失败的即为备。
    注册后根据zookeeper的watch机制监控节点变化，假设节点消失了，就说明主宕机了，其他备这时候再发起注册（为了防止主假死，所有备节点延迟一定时间后再发起注册），注册成功就为新的主，重新提供服务。
    主备方案之所以可行，是以红包的增删改查都是实时的更新数据库为基础。我们不需要像平常的主备方案中维护主备数据一致性，只需要当备切换到主的时候，从数据库重新加载热数据即可。
```
